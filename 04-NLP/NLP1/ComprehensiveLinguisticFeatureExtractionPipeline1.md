Comprehensive Linguistic Feature Extraction Pipeline
Technical Documentation for High-Performance NLP Analysis

Overview
This Python script (optimized-CLE-CSA-R.py) performs comprehensive linguistic feature extraction from diarized speech transcripts. It's specifically optimized to run in parallel with OpenPose video analysis on the Clinamen research platform, requiring careful memory management to prevent GPU memory exhaustion and system overheating.

System Requirements
Recommended Hardware

GPU: NVIDIA GPU with 8GB+ VRAM (RTX 3070 or better)
RAM: 32GB minimum, 64GB recommended
CPU: Multi-core processor (8+ cores recommended)
Storage: 50GB+ free space for models and outputs
Cooling: Adequate cooling system for sustained high GPU/CPU usage

Software Requirements

Python 3.8+
CUDA-compatible GPU drivers
Virtual environment (strongly recommended)


Memory Management Architecture
The script implements five critical memory management strategies to prevent crashes and overheating:
1. GPU Memory Growth Limiting
pythonphysical_devices = tf.config.list_physical_devices('GPU')
if physical_devices:
    for device in physical_devices:
        tf.config.experimental.set_memory_growth(device, True)

Prevents TensorFlow from allocating all GPU memory at startup
Allows OpenPose to run simultaneously
Critical for parallel processing

2. Aggressive Garbage Collection
pythongc.collect()
torch.cuda.empty_cache()
tf.keras.backend.clear_session()

Executed after each speaker view extraction
Executed after each batch save
Forces immediate memory release
Prevents gradual memory accumulation

3. Model Reloading Every 20 Transcripts
pythonif idx % 20 == 0 and idx > 0:
    tf.keras.backend.clear_session()
    self.embeddings['USE'] = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")
    self.embeddings['ELMo'] = hub.load("https://tfhub.dev/google/elmo/2").signatures["default"]

Critical anti-memory-leak measure
TensorFlow models accumulate memory over time
Reloading clears accumulated state
Essential for processing 100+ transcripts

4. Batch Processing with Configurable Size
pythonself.batch_size = batch_size  # Default: 3 transcripts

Small batches (3 transcripts) reduce peak memory
Features saved to disk after each batch
Enables crash recovery
Balances performance vs. stability

5. Text Length Truncation
pythonif word_count > 2000:
    text = ' '.join(text.split()[:2000])

Prevents extremely long texts from causing OOM errors
2000-word limit per speaker view
Safety valve for outlier transcripts


Installation Steps
Step 1: Create Virtual Environment
bash# Navigate to project directory
cd C:\Users\pspre\OneDrive\Desktop\Transcript_Analysis

# Create virtual environment
python -m venv linguistic_env

# Activate environment (Windows)
.\linguistic_env\Scripts\activate

# Activate environment (Mac/Linux)
source linguistic_env/bin/activate
Step 2: Install Dependencies
bash# Core NLP libraries
pip install nltk spacy textblob contractions
pip install numpy pandas scipy

# Deep learning frameworks
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install tensorflow tensorflow-hub

# Transformers and language models
pip install transformers

# Optional: Word2Vec support
pip install gensim

# Download spaCy model
python -m spacy download en_core_web_lg

# Download NLTK data
python -c "import nltk; nltk.download('punkt'); nltk.download('averaged_perceptron_tagger')"
Step 3: Prepare Embeddings Directory
bash# Create embeddings folder
mkdir embeddings
cd embeddings

# Download GloVe 2024 Dolma (300d)
# Required file: glove.2024.dolma.300d.txt
# Place in: embeddings/glove.2024.dolma.300d.txt
```

**Note**: GloVe embeddings must be present or script will fail at startup.

---

## Directory Structure
```
Transcript_Analysis/
â”‚
â”œâ”€â”€ linguistic_env/              # Virtual environment
â”‚
â”œâ”€â”€ embeddings/
â”‚   â””â”€â”€ glove.2024.dolma.300d.txt   # Required
â”‚
â”œâ”€â”€ CSA-research-Xsection-transcripts&OutputOriginal/
â”‚   â”œâ”€â”€ Mac_Audacity_10MinuteConvo_withoutCoordinatorSpeech/
â”‚   â”œâ”€â”€ Mac_Audacity_Grandfather_Passage/
â”‚   â”œâ”€â”€ Mac_Audacity_Picnic_Description/
â”‚   â”œâ”€â”€ Mac_Audacity_Spontaneous_Speech/
â”‚   â”œâ”€â”€ PC_10MinuteConversations_WithoutCoordinatorSpeech/
â”‚   â”œâ”€â”€ PC_Audacity_Grandfather_Passage/
â”‚   â”œâ”€â”€ PC_Audacity_Picnic_Description/
â”‚   â””â”€â”€ PC_Audacity_Spontaneous_Speech/
â”‚
â”œâ”€â”€ output/                     # Generated by script
â”‚   â”œâ”€â”€ comprehensive_extraction.log
â”‚   â”œâ”€â”€ processing_state.json
â”‚   â”œâ”€â”€ features_batch_*.csv
â”‚   â””â”€â”€ comprehensive_linguistic_features.csv  # Final output
â”‚
â””â”€â”€ optimized-CLE-CSA-R.py      # Main script

Running the Script
Test Mode (Recommended First Run)
bashpython optimized-CLE-CSA-R.py \
  --transcripts "CSA-research-Xsection-transcripts&OutputOriginal" \
  --output "output" \
  --test

Processes only 3 transcripts
Validates all dependencies
Tests memory management
Generates comprehensive diagnostics
Estimated time: 10-15 minutes

Limited Run (Validation)
bashpython optimized-CLE-CSA-R.py \
  --transcripts "CSA-research-Xsection-transcripts&OutputOriginal" \
  --output "output" \
  --max_files 10

Processes up to 10 transcripts
Estimated time: 30-45 minutes

Full Production Run
bashpython optimized-CLE-CSA-R.py \
  --transcripts "CSA-research-Xsection-transcripts&OutputOriginal" \
  --output "output" \
  --batch_size 3

Processes all transcripts
Estimated time: 12-24 hours (for 100+ transcripts)
Recommended to run overnight
Monitor GPU temperature


Feature Extraction Pipeline
Stage 1: Metadata Extraction (0.1s per file)

Participant ID parsing
Date extraction
Task type identification
Filename normalization

Stage 2: Diarization Parsing (0.5s per file)

Speaker label detection
Text segmentation by speaker
Creates multiple views:

combined: All speakers
speaker_0: Usually participant
speaker_1: Usually investigator
Additional speakers if present



Stage 3: Feature Extraction Per Speaker (30-60s per view)
A. Lexical Features (1-2s)

Word count, type-token ratio
Brunet's Index
Filled pause detection (um, uh, ah)

B. Sentiment Features (2-3s)

TextBlob polarity analysis
Mean, max, min, stdv sentiment

C. Syntactic/POS Features (3-5s)

25+ part-of-speech frequencies
Verb density, content density
Pronoun and determiner analysis

D. Named Entity Features (2-3s)

Person, location, organization entities
Date and time references
Entity density metrics

E. Specificity Features (1-2s)

Exact vs. vague temporal references
Location specificity
Vague language detection

F. Coherence Features (15-20s per embedding)

GloVe coherence (4-5s)

Intra-window coherence (3, 5, sentence)
Inter-window coherence
Tangentiality slope


USE coherence (5-7s)

Sentence-level embeddings
Universal Sentence Encoder


BERT coherence (5-7s)

Contextual embeddings
CLS token analysis


ELMo coherence (5-7s)

Context-dependent word representations



G. Sentence Probability Features (5-8s)

BERT language model scores
GPT-2 perplexity scores
Quartile analysis

Total per speaker view: 30-60 seconds

Crash Recovery System
Automatic State Saving
json{
  "processed_files": [
    "C:\\Users\\...\\2-002-8-Grandfather-12-19-22-EM_transcript.txt",
    "C:\\Users\\...\\2-013-8-Spontaneous-1-8-24-JD_transcript.txt"
  ],
  "last_update": "2025-01-15T14:32:17.123456"
}
Recovery Behavior

Script checks processing_state.json on startup
Skips already-processed files
Resumes from last unprocessed file
Safe to Ctrl+C and restart

Log Flushing
pythondef flush_logs(self):
    for handler in self.logger.handlers:
        handler.flush()
```
- Logs flushed immediately after each embedding
- Prevents log loss on crash
- Critical for debugging memory issues

---

## Output Format

### Final CSV: `comprehensive_linguistic_features.csv`

#### Metadata Columns (4)
- `participant_id`: e.g., "2-002-8"
- `date`: e.g., "2024-01-08"
- `task_type`: e.g., "Grandfather", "10MinConversation"
- `speaker`: e.g., "combined", "speaker_0", "speaker_1"

#### Feature Columns (~300-400 total)

**Lexical** (6 features)
- `participant_wc`, `participant_types`, `participant_type_token_ratio`
- `participant_brunets_index`
- `participant_ums_or_ahs_count`, `participant_ums_or_ahs_freq`

**Sentiment** (4 features)
- `participant_mean_sentiment`, `participant_max_sentiment`
- `participant_min_sentiment`, `participant_stdv_sentiment`

**Syntactic** (28 features)
- POS frequencies: `participant_nouns_freq`, `participant_verbs_freq`, etc.
- `participant_content_density`

**Named Entities** (10 features)
- `ne_person_count`, `ne_location_count`, `ne_organization_count`
- `ne_person_unique`, `ne_location_unique`
- `ne_person_density`, `ne_location_density`

**Specificity** (4 features)
- `specificity_exact_dates`, `specificity_vague_temporal`
- `specificity_location_count`, `specificity_vague_locations`

**Coherence per Embedding Ã— 4 embeddings** (~48 features each = 192 total)
- Intra-window: `mean_intrawindow_coherence_3_GloVe`, etc.
- Inter-window: `mean_coherence_sentence_USE_interwindow`, etc.
- Tangentiality: `tangentiality_sentence_BERT_interwindow`, etc.

**Sentence Probability** (14 features)
- BERT: `mean_sentence_probability_BERT`, `min_sentence_probability_BERT`, etc.
- GPT2: `mean_sentence_probability_GPT2`, `median_sentence_probability_GPT2`, etc.

---

## Monitoring During Execution

### Real-Time Progress Tracking
```
Processing 45/127: 2-045-8-Grandfather-2-15-24-AB_transcript.txt
  Participant: 2-045-8, Date: 2024-02-15, Task: Grandfather
  Found speakers: ['combined', 'speaker_0', 'speaker_1']
  Extracting features for combined (342 words)...
  Lexical features...
  Sentiment features...
  Coherence features (GloVe)...
  Coherence features (USE)...
  Completed combined, GPU memory cleared
Progress: 45/127 (35.4%) - ETA: 4.2h
GPU Temperature Monitoring
bash# Monitor GPU every 2 seconds (Windows with NVIDIA GPU)
nvidia-smi -l 2

# Watch for:
# - Temperature: Keep below 85Â°C
# - Memory usage: Should stay below 90%
# - Power draw: Normal for your GPU
If Overheating Occurs

Reduce batch size: --batch_size 2 or --batch_size 1
Add delays between files (modify script):

python   time.sleep(5)  # 5-second cooling period

Improve ventilation: Ensure case airflow
Reduce room temperature: Air conditioning recommended


Troubleshooting
Issue: "GloVe file not found"
Solution:
bash# Verify file exists
ls embeddings/glove.2024.dolma.300d.txt

# Check file size (should be ~1-2GB)
Issue: "CUDA out of memory"
Solutions:

Reduce batch size: --batch_size 1
Close other GPU applications
Restart script (uses crash recovery)
Reduce max_files for testing

Issue: Script hangs during coherence extraction
Diagnosis:

Check log file: output/comprehensive_extraction.log
Look for last completed embedding
Most common: TensorFlow memory accumulation

Solution:

Restart script (automatic resume)
Reduce batch size
Ensure model reloading is active (every 20 files)

Issue: "spaCy model not found"
Solution:
bashpython -m spacy download en_core_web_lg
```

### Issue: Memory leak over time
**Expected behavior**: Script reloads TensorFlow models every 20 transcripts
**If still occurring**:
- Check model reload frequency in logs
- Reduce batch size further
- Monitor with `nvidia-smi` and `htop`

---

## Performance Optimization Tips

### For Faster Processing
1. **Use SSD** for transcript storage
2. **Close unnecessary applications**
3. **Disable Windows indexing** on working directories
4. **Use performance power plan** (Windows)
5. **Increase virtual memory** if RAM limited

### For Maximum Stability
1. **Reduce batch size** to 2 or 1
2. **Add cooling breaks** every N files
3. **Monitor temperatures continuously**
4. **Run during coolest time of day**
5. **Use --max_files** for incremental processing

---

## Expected Output Statistics

### Test Run (3 transcripts)
- **Rows**: 6-12 (2-4 speakers per transcript)
- **Columns**: ~350-400 features
- **File size**: ~50-100 KB
- **Time**: 10-15 minutes

### Full Run (100 transcripts)
- **Rows**: 200-400 (multiple speakers per transcript)
- **Columns**: ~350-400 features
- **File size**: 5-15 MB
- **Time**: 12-24 hours

### Diagnostic Output (Test Mode)
```
ðŸ“Š COMPREHENSIVE TEST DIAGNOSTICS
================================================================================

âœ“ Rows processed: 9
âœ“ Total columns: 387
âœ“ Participants: ['2-002-8', '2-013-8', '2-025-8']
âœ“ Task types: ['Grandfather', 'Spontaneous', '10MinConversation']
âœ“ Speakers: ['combined', 'speaker_0', 'speaker_1']

ðŸ“Š DIARIZATION BREAKDOWN:
   combined: 3 rows
   speaker_0: 3 rows
   speaker_1: 3 rows

ðŸ“‹ FEATURE CATEGORIES EXTRACTED:
   Lexical: 6 features, 100.0% coverage
   Sentiment: 4 features, 100.0% coverage
   Syntactic/POS: 28 features, 100.0% coverage
   Named Entities: 10 features, 100.0% coverage
   Specificity: 4 features, 100.0% coverage
   GloVe Coherence: 48 features, 98.5% coverage
   USE Coherence: 48 features, 100.0% coverage
   BERT Coherence: 48 features, 97.2% coverage
   ELMo Coherence: 48 features, 99.1% coverage
   Sentence Probability: 14 features, 100.0% coverage

âœ… ALL FEATURE CATEGORIES EXTRACTED SUCCESSFULLY!
   Ready for full run on all transcripts.

Integration with OpenPose
Parallel Processing Considerations

GPU memory sharing: Both processes use GPU
TensorFlow memory growth: Prevents monopolization
Batch size reduced: Default 3 (vs. 10 standalone)
Process priority: OpenPose should have priority

Recommended Workflow
bash# Terminal 1: Start OpenPose
cd openpose
./build/examples/openpose/openpose.bin --video input.mp4

# Terminal 2: Start linguistic extraction
cd Transcript_Analysis
python optimized-CLE-CSA-R.py --transcripts [...] --output [...]
Resource Monitoring
bash# GPU usage
nvidia-smi -l 2

# CPU/RAM usage
htop  # Linux
Task Manager  # Windows

Citation & References
Models Used

GloVe 2024: Dolma corpus embeddings
Universal Sentence Encoder: Google TensorFlow Hub
BERT: bert-base-uncased, bert-large-cased
ELMo: Allen Institute for AI
GPT-2: OpenAI transformers
spaCy: en_core_web_lg

Papers Referenced

Pennington et al. (2014): GloVe embeddings
Cer et al. (2018): Universal Sentence Encoder
Devlin et al. (2019): BERT
Peters et al. (2018): ELMo
Radford et al. (2019): GPT-2


Final Checklist Before Full Run

 Virtual environment activated
 All dependencies installed (pip list)
 spaCy model downloaded (en_core_web_lg)
 GloVe embeddings present (embeddings/glove.2024.dolma.300d.txt)
 Test run completed successfully (--test)
 GPU temperature monitoring active
 Adequate disk space (50GB+)
 System cooling adequate
 Crash recovery tested (Ctrl+C and restart)
 Log file readable (output/comprehensive_extraction.log)
 Output directory writable


Support & Debugging
Log Analysis
bash# View last 50 lines of log
tail -n 50 output/comprehensive_extraction.log

# Search for errors
grep "ERROR" output/comprehensive_extraction.log

# Find last processed file
grep "Completed" output/comprehensive_extraction.log | tail -n 1
Memory Profiling (Advanced)
bash# Install memory profiler
pip install memory_profiler

# Profile script
python -m memory_profiler optimized-CLE-CSA-R.py [args]

Document Version: 1.0
Last Updated: Based on script analysis
Script: optimized-CLE-CSA-R.py
Compatibility: Python 3.8+, CUDA 11.8+